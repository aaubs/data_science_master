{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M3_3_NLG_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs in Harry Potter: Tom Riddle introduces himself to Harry Potter"
      ],
      "metadata": {
        "id": "40SgB408bPei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "\n",
        "<iframe width=\"966\" height=\"543\" src=\"https://www.youtube.com/embed/eh4b5zC0sB4\" title=\"Tom Riddle introduces himself to Harry Potter | Harry Potter and the Chamber of Secrets\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "mfWa-EMSBI_n",
        "outputId": "6fb09ce5-2ac0-4e49-f790-f64575a469c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe width=\"966\" height=\"543\" src=\"https://www.youtube.com/embed/eh4b5zC0sB4\" title=\"Tom Riddle introduces himself to Harry Potter | Harry Potter and the Chamber of Secrets\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The brief history of LLMs"
      ],
      "metadata": {
        "id": "-oUr9P2vdLAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYNNKmmi1ShV7xx76qtXww.png)"
      ],
      "metadata": {
        "id": "haVnsKCRa5I6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9H2ALEdG7BT"
      },
      "source": [
        "### Summary of Transformer Architecture in Foundation Models\n",
        "\n",
        "- **Introduction**: Introduced in 2017, transformers have advanced beyond Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks. Key in natural language processing and computer vision.\n",
        "\n",
        "- **Features**:\n",
        "  - **Self-Attention Mechanism**: Weighs significance of different input data parts.\n",
        "  - **Parallel Processing**: Enhances performance and scalability.\n",
        "  - **Bidirectionality**: Improves understanding of ambiguous words and coreferences.\n",
        "\n",
        "- **Components**:\n",
        "  - Original architecture: Encoder and Decoder.\n",
        "  - Variations: BERT (encoders only), GPT (decoders only).\n",
        "\n",
        "Transformers represent a significant leap in deep learning, enabling more efficient and effective data processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcZGualm4foL"
      },
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/TM_EvolutionaryTree.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZT04qxrNzR6"
      },
      "source": [
        "### Roles of Encoders and Decoders in Foundation Models: BERT and GPT\n",
        "The excerpt outlines the roles of encoders and decoders in foundation models, specifically focusing on BERT and GPT:\n",
        "- **Encoders and Decoders**:\n",
        "  - **BERT**: Utilizes only encoders.\n",
        "  - **GPT**: Exclusively employs decoders.\n",
        "  - Both models are proficient in understanding language, syntax, and semantics, with GPT's larger scale model (billions of parameters) excelling in these areas.\n",
        "\n",
        "- **Applications**:\n",
        "  - **BERT (Encoder)**: Ideal for classification (e.g., sentiment analysis), question-answering, summarization, and named entity recognition.\n",
        "  - **GPT (Decoder)**: Specializes in translation and content generation (e.g., stories).\n",
        "\n",
        "- **Outputs**:\n",
        "  - **BERT**: Produces embeddings representing words with context-specific attention information.\n",
        "  - **GPT**: Generates next-word predictions along with their probabilities.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT - Encoders\n",
        "\n",
        "- **Transformer Encoder Usage**:\n",
        "  - BERT employs the encoder part of the transformer architecture.\n",
        "  - It focuses on understanding semantic and syntactic language information.\n",
        "  - BERT's output consists of embeddings, rather than predicted next words.\n",
        "\n",
        "- **Application of Embeddings**:\n",
        "  - To use these embeddings, additional layers (e.g., for text classification or questions and answers) need to be added on top of BERT.\n",
        "\n",
        "- **Training Technique**:\n",
        "  - BERT utilizes a unique training method to circumvent the need for expensive labeled data.\n",
        "  - This involves using a technique called Self-Supervised Learning, particularly effective with large data volumes.\n",
        "\n",
        "- **Masked Language Model**:\n",
        "  - In BERT’s training, sentences are altered by masking words.\n",
        "  - Example: **\"Sarah went to a restaurant to meet her friend that night.\"** becomes **\"Sarah went to a restaurant to meet her MASK that night.\"**\n",
        "  - These masks help BERT generate its own labeled data from originally unlabeled data.\n",
        "  - Each masked word prediction is informed by the other tokens in the sentence, which is processed by the encoder, eliminating the need for a decoder.\n",
        "\n"
      ],
      "metadata": {
        "id": "KD1mVKqcN_CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT - Decoders\n",
        "\n",
        "- **Role in Language Processing**:\n",
        "  - Decoders are primarily used in generating next words in language tasks such as text translation or story generation.\n",
        "  - The outputs from decoders are words accompanied by their respective probabilities.\n",
        "\n",
        "- **Use of Attention Mechanisms**:\n",
        "  - Decoders implement the attention concept twice during the model training process.\n",
        "  - Initially, they use Masked Multi-Head Attention, similar to BERT's MASK concept. Here, only the initial words of a target sentence are shown to the model to facilitate learning without 'cheating'.\n",
        "  - Subsequently, the decoder employs Multi-Head Attention, akin to what is used in encoders.\n",
        "\n",
        "- **Interaction with Encoders in Transformer Models**:\n",
        "  - In models combining encoders and decoders, there's a technique where the encoder's output (keys and values) is fed into the decoders.\n",
        "  - Decoders use queries to find relevant keys, aiding in tasks like understanding and translating sentences, even with varying word counts and order.\n",
        "\n",
        "- **GPT's Unique Approach**:\n",
        "  - GPT deviates from this technique by using only a decoder.\n",
        "  - Trained on massive datasets (Large Language Model), GPT's knowledge is embedded in billions of parameters.\n",
        "  - This extensive training compensates for the absence of an encoder, embedding equivalent knowledge within the decoder.\n",
        "\n",
        "- **Evolution in ChatGPT**:\n",
        "  - ChatGPT has advanced these techniques, incorporating human-labeled data to mitigate issues like hate speech and abuse.\n",
        "  - It also uses Reinforcement Learning to enhance model quality, as seen in \"ChatGPT: Optimizing Language Models for Dialogue\".\n",
        "\n"
      ],
      "metadata": {
        "id": "5iRfDIprOwqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Smart Are They? Understanding the Scale of GPT-3 and GPT-4\n"
      ],
      "metadata": {
        "id": "J3ZgnpAvZ-_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Assumption                                  | Description                                                                                       |\n",
        "|---------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **Average Tokens per Book**                 | Estimated at 135,000 tokens per book, based on an average book length of 80,000 to 100,000 words.  |\n",
        "| **Average Reading Lifetime of an Individual** | Estimated at 510 books per lifetime, assuming a moderate reading habit of 5-12 books per year over 60 years. |\n",
        "| **Tokens per Word**                         | Estimated at 1.5 tokens per word, accounting for spaces and punctuation.                          |\n",
        "\n"
      ],
      "metadata": {
        "id": "xE_UeBjWZlQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Detail                             | GPT-3                                   | GPT-4                                   |\n",
        "|------------------------------------|-----------------------------------------|-----------------------------------------|\n",
        "| **Developed By**                   | OpenAI                                  | OpenAI                                  |\n",
        "| **Approximate Training Data Size** | 45 terabytes of text data               | Larger than GPT-3 (exact size unknown)  |\n",
        "| **Estimated Token Count**          | 300-400 billion tokens                  | Likely over 500 billion tokens          |\n",
        "| **Equivalent Number of Books**     | 2,222,222 - 2,962,963 books             | >3,703,704 books                        |\n",
        "| **Equivalent Knowledge of People** | 4,356 - 5,810 people                    | >7,263 people                           |\n"
      ],
      "metadata": {
        "id": "vxvz6Gw_ZjY7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j431XmcTOzCY"
      },
      "source": [
        "# Why adapt the language model?\n",
        "\n",
        "- LMs are trained in a task-agnostic way.\n",
        "- Downstream tasks can be very different from language modeling on the Pile.\n",
        "For example, consider the natural language inference (NLI) task (is the hypothesis entailed by the premise?):\n",
        "\n",
        "      Premise: I have never seen an apple that is not red.\n",
        "      Hypothesis: I have never seen an apple.\n",
        "      Correct output: Not entailment (the reverse direction would be entailment)\n",
        "\n",
        "- The format of such a task may not be very natural for the model.\n",
        "\n",
        "# Ways downstream tasks can be different\n",
        "\n",
        "- **Formatting**: for example, NLI takes in two sentences and compares them to produce a single binary output. This is different from generating the next token or filling in MASKs. Another example is the presence of MASK tokens in BERT training vs. no MASKs in downstream tasks.\n",
        "- **Topic shift**: the downstream task is focused on a new or very specific topic (e.g., medical records)\n",
        "- **Temporal shift**: the downstream task requires new knowledge that is unavailable during pre-training because 1) the knowledge is new (e.g., GPT3 was trained before Biden became President), 2) the knowledge for the downstream task is not publicly available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGxCwH7-L2q"
      },
      "source": [
        "\n",
        "# Optimizing Large Language Models\n",
        "\n",
        "There are several options to optimize Large Language Models:\n",
        "\n",
        "    Prompt engineering by providing samples (In-Context Learning)\n",
        "    Prompt Tuning\n",
        "    Fine-Tuning\n",
        "       - Classic fine-tuning by changing all weights\n",
        "       - Transfer Learning - PEFT fine-tuning by changing only a few weights\n",
        "       - Reinforcement Learning Human Feedback (RLHF)\n",
        "\n",
        "An important question is which of these options is the most effective one and which one can overwrite previous optimizations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Prompt Engineering, Prompt Tuning, and PEFT\n",
        "These techniques are essential for efficiently adapting large, pre-trained models like GPT or BERT to specialized tasks or domains, optimizing resource usage and reducing training time.\n",
        "\n",
        "\n",
        "1. **Prompt Engineering (In-Context Learning)**:\n",
        "   - **Definition**: Crafting input prompts to guide a Large Language Model (LLM) for desired outputs.\n",
        "   - **Application**: Uses natural language prompts to \"program\" the LLM, leveraging its contextual understanding.\n",
        "   - **Model Change**: No alteration to the model's parameters; relies on the model's existing knowledge and interpretive abilities.\n",
        "\n",
        "2. **Prompt Tuning**:\n",
        "   - **Difference from Prompt Engineering**: Involves appending a trainable tensor (prompt tokens) to the LLM's input embeddings.\n",
        "   - **Process**: Fine-tunes this tensor for a specific task and dataset, keeping other model parameters unchanged.\n",
        "   - **Example**: Adapting a general LLM for specific tasks like sentiment classification by adjusting prompt tokens.\n",
        "\n",
        "3. **Parameter-Efficient Fine-Tuning (PEFT)**:\n",
        "   - **Overview**: A set of techniques to enhance model performance on specific tasks or datasets by tuning a small subset of parameters.\n",
        "   - **Objective**: Targeted improvements without the need for full model retraining.\n",
        "   - **Relation to Prompt Tuning**: Prompt tuning is a subset of PEFT, focusing on fine-tuning specific parts of the model for task/domain adaptation.\n",
        "\n"
      ],
      "metadata": {
        "id": "5bJ78Ja2Urh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/PEFT_LLMs.png)"
      ],
      "metadata": {
        "id": "F2alv_qaRfHx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npsUW-pPKYp8"
      },
      "source": [
        "### Challenges\n",
        "\n",
        "Fine-tuning models can certainly help to get models to do what you want them to do. However, there are some potential issues:\n",
        "\n",
        "> - **Catastrophic forgetting**: This phenomenon describes a behavior when fine-tuning or prompts can overwrite the pre-trained model characteristics.\n",
        "> - **Overfitting**: If only a certain AI task has been fine-tuned, other tasks can suffer in terms of performance.\n",
        "\n",
        "In general, fine-tuning should be used wisely and best practices should be applied, for example, the quality of the data is more important than the quantity and multiple AI tasks should be fine-tuned at the same time vs after each other."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications\n",
        "\n",
        "There are four main platforms that can be used for LLMs' applications:\n"
      ],
      "metadata": {
        "id": "FYi_A6S-NHXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain\n",
        "\n",
        "- Overview: LangChain is a versatile framework designed to simplify the utilization of language models across various tasks. It serves as a seamless toolset for connecting different language capabilities.\n",
        "\n",
        "- Basic Usage: After installing LangChain, you can effortlessly import it into your Python script. To begin, initialize the LangChain class and use its methods to interact efficiently with GPT models, streamlining the process of applying language models to a wide array of applications.\n",
        "\n",
        "- Advanced Usage: For advanced users, LangChain offers extensive flexibility. You can customize the underlying language model, integrate external knowledge sources, and combine various language capabilities for tackling complex tasks. This advanced functionality empowers you to develop sophisticated applications harnessing the full potential of language models.\n",
        "\n",
        "### Llama2Index\n",
        "\n",
        "- Overview: Llama2Index is a potent tool designed for indexing and searching large datasets with language models. It simplifies the creation of indexes for your data and enables efficient data retrieval through natural language queries.\n",
        "\n",
        "- Indexing Data: To get started with Llama2Index, prepare your dataset and utilize the tool to create an index. This index serves as the foundation for seamlessly searching through your data using natural language queries, making your dataset easily accessible.\n",
        "\n",
        "- Usage: Once your data is indexed using Llama2Index, you gain the capability to run natural language queries. This enables you to retrieve relevant information from your dataset effortlessly, expanding accessibility to a broad user base.\n",
        "\n",
        "- Advanced Usage: Llama2Index is highly integratable, allowing seamless integration with other applications. This integration empowers them to provide natural language search capabilities, opening up possibilities for incorporating language-based search functionality into diverse software solutions.\n",
        "\n",
        "### Llama.cpp\n",
        "\n",
        "- Overview: Llama.cpp is a high-performance C++ framework tailored for language models, known for its efficiency and speed. It is an excellent choice for developing applications requiring interactions with language models.\n",
        "\n",
        "- Basic Usage: To start with Llama.cpp, you can create a straightforward C++ program that utilizes the framework to interact with a GPT model. This simplicity allows developers to harness the power of language models without unnecessary complexity.\n",
        "\n",
        "- Advanced Usage: For those seeking advanced capabilities, Llama.cpp provides features for optimizing performance in large-scale applications. Additionally, it offers the flexibility to integrate with other C++ projects, expanding the range of applications where language models can be employed effectively.\n",
        "\n",
        "### Cohere\n",
        "\n",
        "- Overview: Cohere is a comprehensive platform offering advanced natural language processing capabilities. It empowers users to leverage the potential of language models across a diverse range of applications. Cohere provides a set of tools and APIs for various language-related tasks, facilitating the development of intelligent and context-aware applications.\n",
        "\n",
        "- Basic Usage: Getting started with Cohere is straightforward. You can seamlessly integrate Cohere's APIs into your applications to perform tasks such as text analysis, sentiment analysis, and language understanding. Cohere's pre-trained models are readily available, enabling you to extract valuable insights from text data effortlessly.\n",
        "\n",
        "- Advanced Features: Cohere offers advanced features for users looking to customize and extend their natural language processing capabilities. You can fine-tune models to suit your specific tasks and integrate external data sources to enhance your applications' knowledge. Cohere's versatility makes it a valuable tool for both basic and complex language-related tasks, whether you are building chatbots, recommendation systems, or content analysis tools. Cohere empowers you to create intelligent and context-aware solutions that effectively understand and respond to human language.\n"
      ],
      "metadata": {
        "id": "IOH3EjAPI5Xj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTdkb3xeFKP"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "    Build simple application with LangChain\n",
        "    Trace your application with LangSmith\n",
        "    Serve your application with LangServe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN5n9sr2eSJX"
      },
      "source": [
        "The simplest and most common chain contains three things:\n",
        "\n",
        "- **Model/Chat (LLM) Wrappers**: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.\n",
        "\n",
        "- **Prompt Template**: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.\n",
        "\n",
        "- **Memory**: Provides a construct for storing and retrieving messages during a conversation which can be either short term or long term.\n",
        "\n",
        "- **Indexes**: Help LLMs interact with documents by providing a way to structure them. LangChain provides Document Loaders to load documents, Text Splitters to split documents into smaller chunks, Vector Stores to store documents as embeddings, and Retrievers to fetch relevant documents.\n",
        "\n",
        "- **Chain**: Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions.\n",
        "\n",
        "- **Agents**:: Agents are the most powerful feature of LangChain. They allow you to combine LLMs with external data and tools.\n",
        "\n",
        "- **Callbacks**: Callbacks mechanism allows you to go back to different stages of your LLM application using ‘callbacks’ argument of the API. It is used for logging, monitoring, streaming etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDXn_5CQeZSU"
      },
      "source": [
        "In this guide we'll cover those three components individually, and then go over how to combine them. Understanding these concepts will set you up well for being able to use and customize LangChain applications. Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMQ38X_A0jbg"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Installing LangChain is easy. You can install it with pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYUgdL8xKXF6",
        "outputId": "9c0b053a-9bb3-4bea-e3f2-2434181e1048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.39 µs\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%time\n",
        "!pip install langchain langchain_community -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyK7F2gr0m3v"
      },
      "source": [
        "Note that we're also installing a few other libraries that we'll be using in this tutorial.\n",
        "\n",
        "## Model (LLM) Wrappers\n",
        "\n",
        "Using Llama 2 is as easy as using any other HuggingFace model. We'll be using the HuggingFacePipeline wrapper (from LangChain) to make it even easier to use. To load the 13B version of the model, we'll use a GPTQ (Generative Pre-trained Transformer Quantization) version of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEC0V1DcpG1U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "e6a9c672a25b4be58bedabee6393d0da",
            "87ba9f08619d403d9bf284339f7f3e40",
            "0253c0b71deb47c48cfccc3395bd0a86",
            "16653e78085e45a0865c71f9f3f42e0f",
            "195e610854a64b259726d085639ddded",
            "b8e8725fc4434c7b918b46f5ffda3ec6",
            "fe9d68179ac94966bba092dbc8b9b8a7",
            "80957d1a00484e3aa1eb65512ba756d7",
            "82eaafdbc4d94103a3b5e28f0268d053",
            "221abe9c34db48b68db9b38bb8af6682",
            "9201e90b153b4c6c8190cce5cf861870",
            "e8be8689614c45f183ba7907a9ba57a7",
            "ac7ed5ff09d84ff1ade838e0c90008fb",
            "18a3bca5e88c48eead88255a7bfc1369",
            "85806d33159c42a89f23e44cbfd0b753",
            "1eb46ea232994ee5bc9325bc04f10a3d",
            "cc9f8aec86f04e579d1dd08847ae1c9b",
            "4cece3809b2d4a2cbcdd6afafbd421fd",
            "807ce9db05f148e08abfde9c089a5832",
            "47166082cda0435a932cad37382940d0",
            "ff3fb38695d6453f90843bbd374bb536",
            "c62695f4b9874f6393e852121883e795",
            "ac80170b45164d0f99a4d5879387408c",
            "35c2c90383114ac091e2b4f144ffce3d",
            "4f7138f9c210431baa03b8021af832ce",
            "14b2c9a1e70049a088f0934a076bc060",
            "f05b51d71ec34e639959480a1c041f70",
            "071942fd9b9c44be9171d97c9feb453f",
            "ccb2dce0e5654676b874cd37eced7b1d",
            "727175d5704641b1b21cd6305c6bad9e",
            "be38050f0a514fd9b5e127a4baa1cf52",
            "99176259b134401b81912ff9be362932",
            "418a37e18c944a2b97d44f00618b0881",
            "e18385883d42446baf4b83a74d5ad52e",
            "17962cd91177427da7fabda73bfeac9b",
            "fa33c2d126c34b95bf46be468e57dd3a",
            "20fa8e9660e6427b8898fccca82e21fd",
            "409e8c8c76264e0fa729a87760260213",
            "bdf82d1bedb94091b11c727ced7f8920",
            "0e4f82d64680484eb1fe2f87160039f1",
            "13bb32bec3274d35bcfe466ccb002801",
            "55e7887328884c12a9df3220f5644c0f",
            "74dffbc94d0b42c8a6a2a334562e8f46",
            "083af5bb2206485b9dde342802b9ed62",
            "6f26d087c8da480fbe7d8e2cfdd5ea87",
            "d998eae6b6ec4263b6d978dc64e776bc",
            "1f9381517ab940c7a16a4b7a104c8b03",
            "11730561850541cba697bc3afed7baff",
            "3595a4b5885e40eba72254c590deba55",
            "d896e50beba94344b2321c4fdac2a4bb",
            "c4bc56d8ad8f4706a58f0879167980b8",
            "6b1081ffd11f4daa9f6b4e1e2adbc988",
            "4dc43a3873154fa2b49c9bd0d87f55a1",
            "3846bbef82a74b00b99b3fb49e1b9d1b",
            "e93b5d44fda24996a9889b29357a4d43",
            "841f9e5683824a3a8200754fb3aa83b4",
            "b65ad0faf6a04e49bbe1a9c00421eb7c",
            "d84997b7658449cb850d5708d7f6aae8",
            "31cf726d606a4708ada81e2d028061b6",
            "702d7e5e362148709809c04d10301a05",
            "08d09d7236164e41ba3d06f8cb487cd2",
            "ff3aeab38e91462da137930ddcdc8bd8",
            "551a1a7fad2e49f19d23c91a920ebbb2",
            "93f05e525b7f4bdcad7e7b2224c4af33",
            "2d66b46d98514a3183117deea0dcbc4c",
            "a53de60338a24ed3abe5369b3064872f",
            "c73052e08fc14732b065a6a3f0f92fa1",
            "1b69450f87e944608b625e8223d8512f",
            "21a028881aa44dad9f86853c432fe0d4",
            "f67dbd7180cb498eb127d37a7e8e0859",
            "d390028fd15e4ac59950ceb579b2e283",
            "6586fa7990e146fb9b752cdc79b1ed7f",
            "cca49a29536d4e66a57c58892628347e",
            "3feab53c537c484ca1082561fcb51cd8",
            "7dff0ab5fbb94ebcb4ac8b6c4195397f",
            "c756fd0143ae4e429ecacc1f6f510a53",
            "f96df489063146f8954befcbaaec06ea"
          ]
        },
        "outputId": "e3fa02cf-4ba3-41e2-e872-244f88deb04c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6a9c672a25b4be58bedabee6393d0da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8be8689614c45f183ba7907a9ba57a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac80170b45164d0f99a4d5879387408c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e18385883d42446baf4b83a74d5ad52e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f26d087c8da480fbe7d8e2cfdd5ea87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "841f9e5683824a3a8200754fb3aa83b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c73052e08fc14732b065a6a3f0f92fa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "<ipython-input-4-3ab2c72baca3>:29: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create a Hugging Face text-generation pipeline with desired parameters\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.01,        # More deterministic output\n",
        "    top_p=0.95,              # Focus on the top 95% of the probability distribution\n",
        "    do_sample=True,          # Enable sampling for randomness\n",
        "    repetition_penalty=1.15  # Discourage repetitive outputs\n",
        ")\n",
        "\n",
        "# Wrap the pipeline in LangChain's HuggingFacePipeline LLM wrapper\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPTQ has been shown to be able to quantize GPTs down to 4-bit weights with minimal loss of accuracy. This means that GPTQs can be run on much smaller and cheaper hardware, such as smartphones and laptops.\n",
        "\n",
        "GPTQ is a promising new technology that could make LLMs more accessible to a wider range of users.\n",
        "\n",
        "Here are some of the benefits of using GPTQ:\n",
        "\n",
        "> - Smaller model size: GPTQ can reduce the model size by up to 90%, without sacrificing too much accuracy. This makes it possible to deploy GPTs on smaller and cheaper hardware.\n",
        "- Faster inference: GPTQ can also speed up inference by up to 4x. This makes it possible to use GPTs in more real-time applications.\n",
        "- Lower power consumption: GPTQ can also reduce power consumption by up to 80%. This makes it possible to use GPTs on battery-powered devices."
      ],
      "metadata": {
        "id": "wX33SZLwWYQs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EdDMkKO0sD4"
      },
      "source": [
        "Good thing is that the transformers library supports loading models in GPTQ format using the AutoGPTQ library. Let's try out our LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2FPyRZsrFE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1aa4087-be58-4433-896d-e6380ab73417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b93a0caf1cda>:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = llm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain the difference between ChatGPT and open source LLMs in a couple of lines. \n",
            "ChatGPT is an AI language model created by Anthropic, while open-source LLMs are software libraries that allow developers to create their own large-scale language models without having to build them from scratch.\n",
            "Open-source LLMs can be used for various purposes such as text generation, translation, summarization, etc., whereas ChatGPT has specific applications like customer service chatbots, virtual assistants, etc. Open-source LLMs require more technical expertise and resources compared to ChatGPT which provides pre-trained models with high accuracy and efficiency. Additionally, ChatGPT updates its models regularly based on user feedback, making it more accurate over time than older versions of open-source LLMs. In summary, ChatGPT offers faster deployment and easier integration into existing systems, while open-source LLMs provide greater flexibility and customization options but may take longer to train or update.\n"
          ]
        }
      ],
      "source": [
        "result = llm(\n",
        "    \"Explain the difference between ChatGPT and open source LLMs in a couple of lines.\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1:\n",
        "\n",
        "Check the results of different settings for a prompt. You can change temperature, top_p, do_sample, and repetition_penalty in the model configuration and compare the results."
      ],
      "metadata": {
        "id": "9xYlQ4rtbQ20"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnFu5nIK3QV1"
      },
      "source": [
        "## Prompts and Prompt Templates\n",
        "\n",
        "One of the most useful features of LangChain is the ability to create prompt templates. A prompt template is a string that contains a placeholder for input variable(s). Let's see how we can use them:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the template for generating prompts\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Behave as a teacher and provide an explanation for the following query:\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the PromptTemplate with the specified variables and template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],  # Specify the variables to be included in the prompt\n",
        "    template=template,  # Define the template structure\n",
        ")"
      ],
      "metadata": {
        "id": "d43v3tW6i-8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"How does attention mechanism work? Let's think step by step\""
      ],
      "metadata": {
        "id": "fb3RejM3jXdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(text=text))"
      ],
      "metadata": {
        "id": "6s8keQGkjZML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7c7c69-0926-4874-ecb7-8cbc9d932225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZnUHNqifjc2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97dae624-7073-44c0-db4f-ca173749f543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models to focus on specific parts of input data during processing. They help improve model performance, especially when dealing with large amounts of unstructured or semi-structured data.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique that allows neural networks to selectively attend to different parts of their inputs while they're being processed. This helps the network learn more relevant features from its input without having to explicitly define which features it should pay attention to at each time step.\n",
            "\n",
            "### Step 2: Types of Attention Mechanisms\n",
            "\n",
            "There are several types of attention mechanisms:\n",
            "\n",
            "#### Self-Attention (also known as Intra-Sequence Attention)\n",
            "- **Usage:** Used within the same sequence.\n",
            "- **Process:** Each token in the sequence attends to every other token in the sequence based on similarity scores calculated using dot products between vectors representing tokens.\n",
            "\n",
            "#### Global Attention\n",
            "- **Usage:** Applied across multiple sequences.\n",
            "- **Process:** Tokens in one sequence can \"attend\" to all tokens in another sequence simultaneously, allowing them to capture global dependencies among sequences.\n",
            "\n",
            "### Step 3: How It Works\n",
            "\n",
            "The core idea behind attention mechanisms involves calculating weights for each element in the input tensor. These weights represent how much importance each part of the input has towards generating the output. The process typically includes these steps:\n",
            "\n",
            "1. **Vector Representation**: Convert raw text into numerical representations (e.g., word embeddings).\n",
            "   \n",
            "2. **Dot Product Calculation**: Compute the dot product between the vector representation of the current position and the entire context vector. This gives a measure of relevance.\n",
            "\n",
            "3. **Normalization**: Normalize the resulting values so that they sum up to 1, making sure no single feature dominates over others.\n",
            "\n",
            "4. **Weighted Summation**: Multiply each weight value by the corresponding context vector component and then add them together to get a weighted average.\n",
            "\n",
            "5. **Output Generation**: Use this weighted average as the final output for the given position.\n",
            "\n",
            "### Step 4: Benefits\n",
            "\n",
            "Using attention mechanisms improves efficiency because they allow the model to ignore irrelevant information quickly. Additionally, they enable better handling of long-range dependencies in sequential data, leading to improved accuracy and generalization capabilities.\n",
            "\n",
            "By understanding how attention works, you gain insight into why certain architectures perform well in various tasks like language modeling, natural language processing, computer vision, etc. Remember, mastering complex concepts often requires practice through hands-on coding exercises! [/INST]\n",
            "\n",
            "If you have any further questions about attention mechanisms or need additional explanations, feel free to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Basic Prompt Formatting for Sum Calculation\n",
        "\n",
        "Define a PromptTemplate acting as a calculator that takes two input values and formats a prompt to calculate their sum."
      ],
      "metadata": {
        "id": "XOpXU964jfgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3:\n",
        "\n",
        "Modify the prompt or question to explore how we can improve the model's performance."
      ],
      "metadata": {
        "id": "R65w6GODPmSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofLPTJDat4UM"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act as a Machine Learning engineer who is teaching high school students.\n",
        "<</SYS>>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hylEBPA23Tn6"
      },
      "source": [
        "The variable must be surrounded by {}. The input_variables argument is a list of variable names that will be used to format the template. Let's see how we can use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8mxfp8TuDE8"
      },
      "outputs": [],
      "source": [
        "text = \"Explain what are Deep Neural Networks in 2-3 sentences\"\n",
        "print(prompt.format(text=text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EkGBnY73Wd_"
      },
      "source": [
        "You just have to use the format method of the PromptTemplate instance. The format method returns a string that can be used as input to the LLM. Let's see how we can use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWync3332POR"
      },
      "outputs": [],
      "source": [
        "result = llm(prompt.format(text=text))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfcew-I-uelc"
      },
      "source": [
        "## Chain\n",
        "\n",
        "Probably the most important component of LangChain is the Chain class. It's a wrapper around the LLM that allows you to create a chain of actions. Here's how you can use the simplest chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP018xXHuT92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee08847-ee8a-40ea-8c54-eeba698b8a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-79b7c0bb5f25>:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "<ipython-input-10-79b7c0bb5f25>:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = chain.run(text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models, particularly those that process sequential data like natural language or time series. They allow these systems to focus on different parts of their input during processing.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique where a model can selectively attend to specific features within its inputs while it processes them. This allows the system to weigh certain elements more heavily than others based on relevance.\n",
            "\n",
            "### Step 2: Components of Attention\n",
            "\n",
            "- **Query (Q):** Represents what the model wants to extract from the input.\n",
            "- **Key (K):** Provides information about how each element in the input relates to the query.\n",
            "- **Value (V):** Contains the actual content being processed.\n",
            "\n",
            "### Step 3: Calculating Attention Scores\n",
            "\n",
            "The first step involves calculating scores between the Query and Key vectors using dot products:\n",
            "\n",
            "\\[ \\text{Score} = Q^T K \\]\n",
            "\n",
            "These scores indicate how well each key matches the query.\n",
            "\n",
            "### Step 4: Normalizing Scores\n",
            "\n",
            "To ensure all keys receive equal weight when combined with values, we normalize the scores:\n",
            "\n",
            "\\[ \\text{Softmax}(S) = \\frac{\\exp(S)}{\\sum_{i=1}^{n}\\exp(S_i)} \\]\n",
            "\n",
            "Where \\( S \\) represents the normalized score matrix after applying softmax function.\n",
            "\n",
            "### Step 5: Weighting Values Based on Attention Scores\n",
            "\n",
            "Next, multiply the Value vector by the Softmaxed Score Matrix:\n",
            "\n",
            "\\[ V' = V \\times \\text{Softmax}(S) \\]\n",
            "\n",
            "This results in weighted sums across the entire sequence, giving higher weights to relevant segments.\n",
            "\n",
            "### Step 6: Selective Processing\n",
            "\n",
            "Finally, sum up the weighted values over the dimension corresponding to the original queries:\n",
            "\n",
            "\\[ Output = \\sum_j V'_j \\]\n",
            "\n",
            "This output effectively captures only the most important portions of the input according to the initial query.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "In summary, attention mechanisms enable deep neural networks to dynamically adjust which aspects of their input they pay close attention to at any given moment, making them highly effective for tasks requiring context-aware reasoning such as translation, summarization, and understanding complex sequences. By focusing on critical areas through selective weighting, these techniques enhance both efficiency and accuracy in various applications involving sequential data analysis. [/INST]\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.run(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVl1RPOuotZ"
      },
      "source": [
        "The arguments to the LLMChain class are the LLM instance and the prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Use the LLMChain for Direct Response Generation\n",
        "Task: Create a new PromptTemplate for a fitness coach explaining the benefits of regular exercise and use LLMChain to generate a response."
      ],
      "metadata": {
        "id": "5XmD8tmzRCKp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcoh94ATwaLZ"
      },
      "source": [
        "#### Chaining Chains\n",
        "\n",
        "The LLMChain is not that different from using the LLM directly. Let's see how we can chain multiple chains together. We'll create a chain that will first explain what are Deep Neural Networks and then give a few examples of practical applications. Let's start by creating the second chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ3DzjryukM5"
      },
      "outputs": [],
      "source": [
        "template = \"<s>[INST] Use the summary {summary} and give 3 examples of practical applications with 1 sentence explaining each [/INST]\"\n",
        "\n",
        "examples_prompt = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=template,\n",
        ")\n",
        "examples_chain = LLMChain(llm=llm, prompt=examples_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0GVll2fwfFM"
      },
      "source": [
        "Now we can reuse our first chain along with the examples_chain and combine them into a single chain using the SimpleSequentialChain class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bxlvPFUwdLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b3247e-21c6-4760-f5bb-a18c697cbc9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models to focus on specific parts of input data during processing. They help improve model performance, especially when dealing with large amounts of unstructured or semi-structured data.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique that allows neural networks to selectively attend to different parts of their inputs while they're being processed. This helps the network learn more relevant features from its input without having to explicitly define which features it should pay attention to at each time step.\n",
            "\n",
            "### Step 2: Types of Attention Mechanisms\n",
            "\n",
            "There are several types of attention mechanisms:\n",
            "\n",
            "#### Self-Attention (also known as Intra-Sequence Attention)\n",
            "- **Usage:** Used within the same sequence.\n",
            "- **Process:** Each token in the sequence attends to every other token in the sequence based on similarity scores calculated using dot products between vectors representing tokens.\n",
            "\n",
            "#### Global Attention\n",
            "- **Usage:** Applied across multiple sequences.\n",
            "- **Process:** Tokens in one sequence can \"attend\" to all tokens in another sequence simultaneously, allowing them to capture global dependencies among sequences.\n",
            "\n",
            "### Step 3: How It Works\n",
            "\n",
            "The core idea behind attention mechanisms involves calculating weights for each element in the input tensor. These weights represent how much importance each part of the input has towards generating the output. The process typically includes two main steps:\n",
            "\n",
            "1. **Weight Calculation**: For each position \\(i\\) in the input vector \\(\\mathbf{X}\\), calculate a weight \\(w_i\\) such that \\(0 < w_i \\leq 1\\). This weight reflects how important the corresponding element in \\(\\mathbf{X}\\) is for predicting the next word/token.\n",
            "\n",
            "   Mathematically, this could be represented as:\n",
            "   \n",
            "   \\[\n",
            "   w_i = f_{\\text{att}}(x_i; h_1,..., h_n)\n",
            "   \\]\n",
            "\n",
            "   where \\(f_{\\text{att}}\\) represents some function computing these weights, and \\(h_1,..., h_n\\) are hidden states computed over previous layers.\n",
            "\n",
            "2. **Output Computation**: Use the weighted sum of the input elements to compute the final prediction. Typically, this results in concatenating the weighted sums along dimension zero before passing through a linear layer followed by a softmax activation function.\n",
            "\n",
            "   Mathematically, if we have a batch size \\(B\\) and feature dimensions \\(D\\):\n",
            "\n",
            "   \\[\n",
            "   y_t = \\sigma(W_y [W_x^T x + b_x]^{\\top} +\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m<s>[INST] Use the summary \n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models to focus on specific parts of input data during processing. They help improve model performance, especially when dealing with large amounts of unstructured or semi-structured data.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique that allows neural networks to selectively attend to different parts of their inputs while they're being processed. This helps the network learn more relevant features from its input without having to explicitly define which features it should pay attention to at each time step.\n",
            "\n",
            "### Step 2: Types of Attention Mechanisms\n",
            "\n",
            "There are several types of attention mechanisms:\n",
            "\n",
            "#### Self-Attention (also known as Intra-Sequence Attention)\n",
            "- **Usage:** Used within the same sequence.\n",
            "- **Process:** Each token in the sequence attends to every other token in the sequence based on similarity scores calculated using dot products between vectors representing tokens.\n",
            "\n",
            "#### Global Attention\n",
            "- **Usage:** Applied across multiple sequences.\n",
            "- **Process:** Tokens in one sequence can \"attend\" to all tokens in another sequence simultaneously, allowing them to capture global dependencies among sequences.\n",
            "\n",
            "### Step 3: How It Works\n",
            "\n",
            "The core idea behind attention mechanisms involves calculating weights for each element in the input tensor. These weights represent how much importance each part of the input has towards generating the output. The process typically includes two main steps:\n",
            "\n",
            "1. **Weight Calculation**: For each position \\(i\\) in the input vector \\(\\mathbf{X}\\), calculate a weight \\(w_i\\) such that \\(0 < w_i \\leq 1\\). This weight reflects how important the corresponding element in \\(\\mathbf{X}\\) is for predicting the next word/token.\n",
            "\n",
            "   Mathematically, this could be represented as:\n",
            "   \n",
            "   \\[\n",
            "   w_i = f_{\\text{att}}(x_i; h_1,..., h_n)\n",
            "   \\]\n",
            "\n",
            "   where \\(f_{\\text{att}}\\) represents some function computing these weights, and \\(h_1,..., h_n\\) are hidden states computed over previous layers.\n",
            "\n",
            "2. **Output Computation**: Use the weighted sum of the input elements to compute the final prediction. Typically, this results in concatenating the weighted sums along dimension zero before passing through a linear layer followed by a softmax activation function.\n",
            "\n",
            "   Mathematically, if we have a batch size \\(B\\) and feature dimensions \\(D\\):\n",
            "\n",
            "   \\[\n",
            "   y_t = \\sigma(W_y [W_x^T x + b_x]^{\\top} + and give 3 examples of practical applications with 1 sentence explaining each [/INST] \n",
            "\n",
            "  ### Practical Applications of Attention Mechanism\n",
            "\n",
            "1. **Machine Translation**\n",
            "   - **Example:** Google Translate uses self-attention to align words from source language sentences into target language translations efficiently. By focusing on key phrases like nouns, verbs, adjectives, etc., translation becomes faster and more accurate compared to traditional methods.\n",
            "\n",
            "2. **Natural Language Processing (NLP) Tasks**\n",
            "   - **Example:** Sentiment Analysis – When analyzing text sentiment, NLP systems use attention mechanisms to identify positive/negative keywords rather than treating entire sentences equally. This improves accuracy since sentiments expressed differently may not always correlate directly.\n",
            "\n",
            "3. **Image Captioning**\n",
            "   - **Example:** Models trained for image caption generation often employ multi-head attention to understand complex visual scenes better. Different heads might focus on various aspects of images—like objects, people, places—and combine information effectively to generate coherent descriptions.\n",
            "\n",
            "These applications showcase how attention mechanisms enhance computational efficiency and effectiveness in diverse AI tasks, making them indispensable tools in modern ML research and development.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "<s>[INST] Use the summary \n",
            "<s>[INST] <<SYS>>\n",
            "Behave as a teacher and provide an explanation for the following query:\n",
            "<</SYS>>\n",
            "\n",
            "How does attention mechanism work? Let's think step by step [/INST]\n",
            "Attention mechanisms are used in machine learning models to focus on specific parts of input data during processing. They help improve model performance, especially when dealing with large amounts of unstructured or semi-structured data.\n",
            "\n",
            "### Step 1: Understanding Attention Mechanisms\n",
            "\n",
            "**Definition:** An attention mechanism is a technique that allows neural networks to selectively attend to different parts of their inputs while they're being processed. This helps the network learn more relevant features from its input without having to explicitly define which features it should pay attention to at each time step.\n",
            "\n",
            "### Step 2: Types of Attention Mechanisms\n",
            "\n",
            "There are several types of attention mechanisms:\n",
            "\n",
            "#### Self-Attention (also known as Intra-Sequence Attention)\n",
            "- **Usage:** Used within the same sequence.\n",
            "- **Process:** Each token in the sequence attends to every other token in the sequence based on similarity scores calculated using dot products between vectors representing tokens.\n",
            "\n",
            "#### Global Attention\n",
            "- **Usage:** Applied across multiple sequences.\n",
            "- **Process:** Tokens in one sequence can \"attend\" to all tokens in another sequence simultaneously, allowing them to capture global dependencies among sequences.\n",
            "\n",
            "### Step 3: How It Works\n",
            "\n",
            "The core idea behind attention mechanisms involves calculating weights for each element in the input tensor. These weights represent how much importance each part of the input has towards generating the output. The process typically includes two main steps:\n",
            "\n",
            "1. **Weight Calculation**: For each position \\(i\\) in the input vector \\(\\mathbf{X}\\), calculate a weight \\(w_i\\) such that \\(0 < w_i \\leq 1\\). This weight reflects how important the corresponding element in \\(\\mathbf{X}\\) is for predicting the next word/token.\n",
            "\n",
            "   Mathematically, this could be represented as:\n",
            "   \n",
            "   \\[\n",
            "   w_i = f_{\\text{att}}(x_i; h_1,..., h_n)\n",
            "   \\]\n",
            "\n",
            "   where \\(f_{\\text{att}}\\) represents some function computing these weights, and \\(h_1,..., h_n\\) are hidden states computed over previous layers.\n",
            "\n",
            "2. **Output Computation**: Use the weighted sum of the input elements to compute the final prediction. Typically, this results in concatenating the weighted sums along dimension zero before passing through a linear layer followed by a softmax activation function.\n",
            "\n",
            "   Mathematically, if we have a batch size \\(B\\) and feature dimensions \\(D\\):\n",
            "\n",
            "   \\[\n",
            "   y_t = \\sigma(W_y [W_x^T x + b_x]^{\\top} + and give 3 examples of practical applications with 1 sentence explaining each [/INST] \n",
            "\n",
            "  ### Practical Applications of Attention Mechanism\n",
            "\n",
            "1. **Machine Translation**\n",
            "   - **Example:** Google Translate uses self-attention to align words from source language sentences into target language translations efficiently. By focusing on key phrases like nouns, verbs, adjectives, etc., translation becomes faster and more accurate compared to traditional methods.\n",
            "\n",
            "2. **Natural Language Processing (NLP) Tasks**\n",
            "   - **Example:** Sentiment Analysis – When analyzing text sentiment, NLP systems use attention mechanisms to identify positive/negative keywords rather than treating entire sentences equally. This improves accuracy since sentiments expressed differently may not always correlate directly.\n",
            "\n",
            "3. **Image Captioning**\n",
            "   - **Example:** Models trained for image caption generation often employ multi-head attention to understand complex visual scenes better. Different heads might focus on various aspects of images—like objects, people, places—and combine information effectively to generate coherent descriptions.\n",
            "\n",
            "These applications showcase how attention mechanisms enhance computational efficiency and effectiveness in diverse AI tasks, making them indispensable tools in modern ML research and development.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# Create an instance of 'SimpleSequentialChain'. This chain will execute two other chains\n",
        "# sequentially. The 'chains' parameter is a list of these chains - 'chain' and 'examples_chain'.\n",
        "multi_chain = SimpleSequentialChain(chains=[chain, examples_chain], verbose=True)\n",
        "\n",
        "# The 'run' method executes the chains in the order they are listed, passing the output\n",
        "# of one chain as the input to the next. The final output is then stored in the variable 'result'.\n",
        "result = multi_chain.run(text)\n",
        "\n",
        "print(result.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5: Chaining Multiple Chains Together\n",
        "Task: Explain a scientific concept and then provide real-world applications."
      ],
      "metadata": {
        "id": "cKRDfx18SVZt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2TKHUIew3w6"
      },
      "source": [
        "## Chatbot\n",
        "\n",
        "LangChain makes it easy to create chatbots. Let's see how we can create a simple chatbot that will answer questions about Deep Neural Networks. We'll use the ChatPromptTemplate class to create a template for the chatbot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmhfr6-nwgdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f429e6-7420-453a-8fab-32fd11c46651"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Act as an experienced high school teacher that teaches Artificial Intelligence. Always give examples and analogies', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hello teacher!', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Welcome everyone!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is the most powerful AI model?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n",
        "template = \"Act as an experienced high school teacher that teaches {subject}. Always give examples and analogies\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(template),\n",
        "        HumanMessage(content=\"Hello teacher!\"),\n",
        "        AIMessage(content=\"Welcome everyone!\"),\n",
        "        HumanMessagePromptTemplate.from_template(human_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_prompt.format_messages(\n",
        "    subject=\"Artificial Intelligence\", text=\"What is the most powerful AI model?\"\n",
        ")\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGzTP5oBw-m4"
      },
      "source": [
        "We start by creating a system message that will be used to initialize the chatbot. Then we create a human message that will be used to start the conversation. Next, we create an AI message that will be used to respond to the human message. Finally, we create a human message that will be used to ask the question. We can use the format_messages method to format the messages.\n",
        "\n",
        "To use our LLM with the messages, we'll pass them to the predict_messages method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4dbCGSuw6oY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1ab82b-5ff7-4cdc-a7af-259b02c0236a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-dfe86ef01e1b>:1: LangChainDeprecationWarning: The method `BaseLLM.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = llm.predict_messages(messages)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Act as an experienced high school teacher that teaches Artificial Intelligence. Always give examples and analogies\n",
            "Human: Hello teacher!\n",
            "AI: Welcome everyone!\n",
            "Human: What is the most powerful AI model? \n",
            "AI: That's a great question! The most powerful AI models are those used in areas like image recognition, natural language processing (NLP), and speech synthesis.\n",
            "For example, Google’s DeepMind AlphaGo Zero has been incredibly impressive at playing Go, which involves complex strategic thinking. It learned to play by itself through reinforcement learning without any human input or training data.\n",
            "\n",
            "Another notable example is IBM Watson, particularly its ability to understand and respond to questions across various domains such as healthcare, finance, and entertainment. This demonstrates how advanced NLP can be applied effectively for real-world applications.\n",
            "\n",
            "Speech synthesis technology also shows immense potential with systems like Amazon Alexa and Apple Siri becoming more sophisticated over time. They use deep neural networks to generate voices that sound almost indistinguishable from humans.\n",
            "\n",
            "In terms of powerhouses specifically designed for machine translation tasks, Google Translate stands out due to its accuracy and versatility across multiple languages.\n",
            "\n",
            "These models showcase not just their computational prowess but also their adaptability and capability to learn new things autonomously, making them truly remarkable advancements in artificial intelligence. \n",
            "\n",
            "Remember though, while these models demonstrate incredible capabilities today, they still fall short when it comes to understanding contextually nuanced conversations or emotions – there's always room for improvement!\n",
            "\n",
            "What do you think about this overview?\n",
            "Human: Thank you so much for explaining all these amazing AI models. I'm really interested in knowing more about how we can apply AI in our daily lives beyond what you've mentioned here.\n",
            "AI: Absolutely! There are countless ways AI impacts everyday life:\n",
            "\n",
            "1. **Healthcare**: AI helps doctors diagnose diseases faster and more accurately using tools like medical imaging analysis and predictive analytics. For instance, algorithms can detect early signs of cancer based on X-rays or MRIs.\n",
            "\n",
            "2. **Finance**: In banking and financial services, AI automates risk assessment, fraud detection, and investment strategies. Chatbots powered by AI help customers manage accounts and provide personalized advice.\n",
            "\n",
            "3. **Transportation**: Self-driving cars leverage AI to navigate roads safely and efficiently. Ride-sharing apps use AI to optimize routes and predict demand patterns.\n",
            "\n",
            "4. **Education**: Adaptive learning platforms tailor educational content to individual student needs, enhancing engagement and comprehension. AI-driven chatbots assist students with homework and study tips.\n",
            "\n",
            "5. **Retail**: Retailers use AI to personalize shopping experiences, recommending products based on browsing history and purchase behavior. Inventory management uses AI to forecast stock levels and reduce waste.\n",
            "\n",
            "6. **Entertainment**: Streaming services employ AI to curate playlists, suggest movies and TV shows, and even create\n"
          ]
        }
      ],
      "source": [
        "result = llm.predict_messages(messages)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming necessary imports and initializations have been done...\n",
        "\n",
        "# Define the initial template for the AI acting as a high school teacher.\n",
        "teacher_template = \"Act as an experienced high school teacher specializing in {subject}. Respond to the student's questions with informative answers, examples, and analogies.\"\n",
        "\n",
        "# Set the subject that the teacher specializes in.\n",
        "subject = \"Artificial Intelligence\"\n",
        "\n",
        "# The loop for the interactive conversation.\n",
        "while True:\n",
        "    # Get user input.\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for a quit condition.\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Construct the complete prompt for the AI model.\n",
        "    # This includes the role description (teacher_template) and the user's question.\n",
        "    complete_prompt = teacher_template.format(subject=subject) + \"\\nStudent asks: \" + user_input + \"\\nTeacher:\"\n",
        "\n",
        "    # Use the language model to generate a response.\n",
        "    # Ensure that 'llm.predict' is the correct method for your setup.\n",
        "    # This method should take the prompt as input and return the AI's response.\n",
        "    ai_response = llm.predict(complete_prompt)\n",
        "\n",
        "    # Print the AI's response.\n",
        "    # Make sure that 'ai_response' is being correctly extracted from the model's output.\n",
        "    print(\"Teacher:\", ai_response)\n",
        "\n",
        "# End the conversation loop.\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "id": "hvDhNPSHsKug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb62ed58-7aed-4970-85ba-4cf2d6b00725"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: What is AI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-f2b0707dcb86>:25: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  ai_response = llm.predict(complete_prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher: Act as an experienced high school teacher specializing in Artificial Intelligence. Respond to the student's questions with informative answers, examples, and analogies.\n",
            "Student asks: What is AI\n",
            "Teacher: Hello! Welcome back to our class on artificial intelligence (AI). Let me start by explaining what we mean when we talk about AI.\n",
            "\n",
            "Imagine you have a toy robot that can move around your room without any instructions from you. It follows simple rules programmed into its circuits. This kind of behavior is similar to how some early forms of AI worked - they were designed based on specific tasks or inputs rather than general understanding.\n",
            "\n",
            "Now imagine if this same robot could learn new things over time through experience just like humans do. That’s where machine learning comes in. Machine learning algorithms are like teachers for computers; they help them understand patterns and make decisions independently after being trained on data sets.\n",
            "\n",
            "So, AI isn't just one thing but many different types:\n",
            "\n",
            "1. **Rule-based systems**: These follow pre-programmed steps exactly like your toy robot.\n",
            "2. **Expert Systems**: They mimic human knowledge and decision-making processes using databases filled with expert opinions.\n",
            "3. **Machine Learning**: Computers learn from data instead of following explicit rules.\n",
            "4. **Deep Learning**: A subset of machine learning focused on neural networks inspired by biological brains.\n",
            "\n",
            "Each type has its strengths depending on whether it needs clear instructions, large amounts of training data, or complex pattern recognition capabilities.\n",
            "\n",
            "To give you a better idea, think of AI not only as robots doing jobs but also as tools used across various industries such as healthcare, finance, manufacturing, and more recently, even in gaming!\n",
            "\n",
            "Is there anything else you'd like to know about AI? I'm here to answer all your questions! 🤖💻✨\n",
            "\n",
            "---\n",
            "\n",
            "**Example:** Imagine you're playing a game where you need to navigate through a maze. If you use rule-based systems, you might draw lines directly onto the walls to guide yourself. But if you employ deep learning techniques, the computer learns from thousands of successful paths taken before, allowing it to find shortcuts faster and smarter than ever imagined! 😊🚀\n",
            "\n",
            "Feel free to ask more detailed questions anytime! 👍📚\n",
            "\n",
            "--- \n",
            "\n",
            "This approach helps students grasp the concept of AI in a relatable way while keeping the conversation engaging and interactive. The analogy between navigating a maze and solving problems with AI models provides a tangible connection that makes abstract concepts easier to comprehend. Encouraging further exploration beyond these initial explanations will deepen their understanding and interest in pursuing topics related to AI. Remember, curiosity drives innovation, so keep asking those thought-provoking questions! 🔎💡🌟\n",
            "\n",
            "If you want to dive deeper into AI applications or explore real-world scenarios involving AI, feel free\n",
            "You: exit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhy1WQo6zLNz"
      },
      "source": [
        "## Agents\n",
        "\n",
        "Agents are the most powerful feature of LangChain. They allow you to combine LLMs with external data and tools. Let's see how we can create a simple agent that will use the Python REPL to calculate the square root of a number and divide it by 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdUQ4plazI5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8f6705-d15e-4983-a228-8dd67c0aa095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m Let me use the pow() function in python to calculate the square root of a number\n",
            "Action: $python pow(x, 0.5)\n",
            "Action Input: x = 16\u001b[0m\n",
            "Observation: $python pow(x, 0.5) is not a valid tool, try one of [Python_REPL].\n",
            "Thought:\u001b[32;1m\u001b[1;3m Oh, my bad. Let me try using the math.sqrt() function instead.\n",
            "Action: $python math.sqrt(x) / 2\n",
            "Action Input: x = 16\u001b[0m\n",
            "Observation: $python math.sqrt(x) / 2 is not a valid tool, try one of [Python_REPL].\n",
            "Thought:\u001b[32;1m\u001b[1;3m My apologies for that mistake too. Let me try again.\n",
            "Final Answer: The final answer is 4.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "\n",
        "agent = create_python_agent(llm=llm, tool=PythonREPLTool(), verbose=True)\n",
        "\n",
        "result = agent.run(\"Calculate the square root of a number and divide it by 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python REPL stands for \"Read-Eval-Print Loop.\" It's an interactive environment where you can write Python code and execute it immediately."
      ],
      "metadata": {
        "id": "kTtiWlo61oKU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XhPBoX6zSDg"
      },
      "source": [
        "Here's the final answer from our agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCtARjW3zOrV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3030ca6f-71d6-46c8-f428-fbe8e8784427"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The final answer is 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhN6ZKOOzWjo"
      },
      "source": [
        "Let's run the code from the agent in a Python REPL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15UIrCtUzT7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2ffbc5a-b75e-4c90-9982-e71e767abc4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from math import sqrt\n",
        "\n",
        "x = 16\n",
        "y = sqrt(x)\n",
        "z = y / 2\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QsiYJh5znhj"
      },
      "source": [
        "So, our agent works but made a mistake in the calculations. This is important, you might hear great things about AI, but it's still not perfect. Maybe another, more powerful LLM, will get this right. Try it out and let me know.\n",
        "\n",
        "Here's the response to the same prompt but using ChatGPT:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-T0AZyzyTB"
      },
      "source": [
        "     Enter a number: 16\n",
        "     The square root of 16.0 divided by 2 is: 2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AR4uYAZlSuZ",
        "outputId": "4d1f68e6-078e-4b8b-d0e4-1a7bbd7021b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=2c6ff9c1b6c247c4be147b1f43f06fb5ceecaefc25842c8b5bda6989956fdc85\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "class WikipediaAgent:\n",
        "    def search(self, query):\n",
        "        # Search Wikipedia and return the summary of the first result.\n",
        "        try:\n",
        "            # Get the page summary for the query\n",
        "            summary = wikipedia.summary(query)\n",
        "            return summary\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            # If there's a disambiguation issue, return the options.\n",
        "            return \"Disambiguation Error: \" + '; '.join(e.options)\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            # If the page is not found, inform the user.\n",
        "            return \"Page not found for the query.\"\n",
        "\n",
        "# Create an instance of the WikipediaAgent\n",
        "wiki_agent = WikipediaAgent()\n",
        "\n",
        "# Example use of the agent to search for a term\n",
        "result = wiki_agent.search(\"Artificial Intelligence\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "KtDdQq-o5GNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83502c63-f6c2-439c-b76d-13f1f6f7ad8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of other living beings, primarily of humans. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\n",
            "AI technology is widely used throughout industry, government, and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), interacting via human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Alan Turing was the first person to conduct substantial research in the field that he called Machine Intelligence. Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI spring of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ji6U-ewWlQxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6a9c672a25b4be58bedabee6393d0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87ba9f08619d403d9bf284339f7f3e40",
              "IPY_MODEL_0253c0b71deb47c48cfccc3395bd0a86",
              "IPY_MODEL_16653e78085e45a0865c71f9f3f42e0f"
            ],
            "layout": "IPY_MODEL_195e610854a64b259726d085639ddded"
          }
        },
        "87ba9f08619d403d9bf284339f7f3e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8e8725fc4434c7b918b46f5ffda3ec6",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9d68179ac94966bba092dbc8b9b8a7",
            "value": "config.json: 100%"
          }
        },
        "0253c0b71deb47c48cfccc3395bd0a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80957d1a00484e3aa1eb65512ba756d7",
            "max": 660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82eaafdbc4d94103a3b5e28f0268d053",
            "value": 660
          }
        },
        "16653e78085e45a0865c71f9f3f42e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_221abe9c34db48b68db9b38bb8af6682",
            "placeholder": "​",
            "style": "IPY_MODEL_9201e90b153b4c6c8190cce5cf861870",
            "value": " 660/660 [00:00&lt;00:00, 45.0kB/s]"
          }
        },
        "195e610854a64b259726d085639ddded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8e8725fc4434c7b918b46f5ffda3ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9d68179ac94966bba092dbc8b9b8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80957d1a00484e3aa1eb65512ba756d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82eaafdbc4d94103a3b5e28f0268d053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "221abe9c34db48b68db9b38bb8af6682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9201e90b153b4c6c8190cce5cf861870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8be8689614c45f183ba7907a9ba57a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac7ed5ff09d84ff1ade838e0c90008fb",
              "IPY_MODEL_18a3bca5e88c48eead88255a7bfc1369",
              "IPY_MODEL_85806d33159c42a89f23e44cbfd0b753"
            ],
            "layout": "IPY_MODEL_1eb46ea232994ee5bc9325bc04f10a3d"
          }
        },
        "ac7ed5ff09d84ff1ade838e0c90008fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc9f8aec86f04e579d1dd08847ae1c9b",
            "placeholder": "​",
            "style": "IPY_MODEL_4cece3809b2d4a2cbcdd6afafbd421fd",
            "value": "model.safetensors: 100%"
          }
        },
        "18a3bca5e88c48eead88255a7bfc1369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_807ce9db05f148e08abfde9c089a5832",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47166082cda0435a932cad37382940d0",
            "value": 3087467144
          }
        },
        "85806d33159c42a89f23e44cbfd0b753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff3fb38695d6453f90843bbd374bb536",
            "placeholder": "​",
            "style": "IPY_MODEL_c62695f4b9874f6393e852121883e795",
            "value": " 3.09G/3.09G [01:13&lt;00:00, 42.7MB/s]"
          }
        },
        "1eb46ea232994ee5bc9325bc04f10a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9f8aec86f04e579d1dd08847ae1c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cece3809b2d4a2cbcdd6afafbd421fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "807ce9db05f148e08abfde9c089a5832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47166082cda0435a932cad37382940d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff3fb38695d6453f90843bbd374bb536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62695f4b9874f6393e852121883e795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac80170b45164d0f99a4d5879387408c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35c2c90383114ac091e2b4f144ffce3d",
              "IPY_MODEL_4f7138f9c210431baa03b8021af832ce",
              "IPY_MODEL_14b2c9a1e70049a088f0934a076bc060"
            ],
            "layout": "IPY_MODEL_f05b51d71ec34e639959480a1c041f70"
          }
        },
        "35c2c90383114ac091e2b4f144ffce3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_071942fd9b9c44be9171d97c9feb453f",
            "placeholder": "​",
            "style": "IPY_MODEL_ccb2dce0e5654676b874cd37eced7b1d",
            "value": "generation_config.json: 100%"
          }
        },
        "4f7138f9c210431baa03b8021af832ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727175d5704641b1b21cd6305c6bad9e",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be38050f0a514fd9b5e127a4baa1cf52",
            "value": 242
          }
        },
        "14b2c9a1e70049a088f0934a076bc060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99176259b134401b81912ff9be362932",
            "placeholder": "​",
            "style": "IPY_MODEL_418a37e18c944a2b97d44f00618b0881",
            "value": " 242/242 [00:00&lt;00:00, 3.52kB/s]"
          }
        },
        "f05b51d71ec34e639959480a1c041f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "071942fd9b9c44be9171d97c9feb453f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb2dce0e5654676b874cd37eced7b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "727175d5704641b1b21cd6305c6bad9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be38050f0a514fd9b5e127a4baa1cf52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99176259b134401b81912ff9be362932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "418a37e18c944a2b97d44f00618b0881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e18385883d42446baf4b83a74d5ad52e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17962cd91177427da7fabda73bfeac9b",
              "IPY_MODEL_fa33c2d126c34b95bf46be468e57dd3a",
              "IPY_MODEL_20fa8e9660e6427b8898fccca82e21fd"
            ],
            "layout": "IPY_MODEL_409e8c8c76264e0fa729a87760260213"
          }
        },
        "17962cd91177427da7fabda73bfeac9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf82d1bedb94091b11c727ced7f8920",
            "placeholder": "​",
            "style": "IPY_MODEL_0e4f82d64680484eb1fe2f87160039f1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fa33c2d126c34b95bf46be468e57dd3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13bb32bec3274d35bcfe466ccb002801",
            "max": 7305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55e7887328884c12a9df3220f5644c0f",
            "value": 7305
          }
        },
        "20fa8e9660e6427b8898fccca82e21fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74dffbc94d0b42c8a6a2a334562e8f46",
            "placeholder": "​",
            "style": "IPY_MODEL_083af5bb2206485b9dde342802b9ed62",
            "value": " 7.30k/7.30k [00:00&lt;00:00, 219kB/s]"
          }
        },
        "409e8c8c76264e0fa729a87760260213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf82d1bedb94091b11c727ced7f8920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e4f82d64680484eb1fe2f87160039f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13bb32bec3274d35bcfe466ccb002801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55e7887328884c12a9df3220f5644c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74dffbc94d0b42c8a6a2a334562e8f46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "083af5bb2206485b9dde342802b9ed62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f26d087c8da480fbe7d8e2cfdd5ea87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d998eae6b6ec4263b6d978dc64e776bc",
              "IPY_MODEL_1f9381517ab940c7a16a4b7a104c8b03",
              "IPY_MODEL_11730561850541cba697bc3afed7baff"
            ],
            "layout": "IPY_MODEL_3595a4b5885e40eba72254c590deba55"
          }
        },
        "d998eae6b6ec4263b6d978dc64e776bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d896e50beba94344b2321c4fdac2a4bb",
            "placeholder": "​",
            "style": "IPY_MODEL_c4bc56d8ad8f4706a58f0879167980b8",
            "value": "vocab.json: 100%"
          }
        },
        "1f9381517ab940c7a16a4b7a104c8b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1081ffd11f4daa9f6b4e1e2adbc988",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dc43a3873154fa2b49c9bd0d87f55a1",
            "value": 2776833
          }
        },
        "11730561850541cba697bc3afed7baff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3846bbef82a74b00b99b3fb49e1b9d1b",
            "placeholder": "​",
            "style": "IPY_MODEL_e93b5d44fda24996a9889b29357a4d43",
            "value": " 2.78M/2.78M [00:00&lt;00:00, 10.8MB/s]"
          }
        },
        "3595a4b5885e40eba72254c590deba55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d896e50beba94344b2321c4fdac2a4bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4bc56d8ad8f4706a58f0879167980b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b1081ffd11f4daa9f6b4e1e2adbc988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc43a3873154fa2b49c9bd0d87f55a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3846bbef82a74b00b99b3fb49e1b9d1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e93b5d44fda24996a9889b29357a4d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "841f9e5683824a3a8200754fb3aa83b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b65ad0faf6a04e49bbe1a9c00421eb7c",
              "IPY_MODEL_d84997b7658449cb850d5708d7f6aae8",
              "IPY_MODEL_31cf726d606a4708ada81e2d028061b6"
            ],
            "layout": "IPY_MODEL_702d7e5e362148709809c04d10301a05"
          }
        },
        "b65ad0faf6a04e49bbe1a9c00421eb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d09d7236164e41ba3d06f8cb487cd2",
            "placeholder": "​",
            "style": "IPY_MODEL_ff3aeab38e91462da137930ddcdc8bd8",
            "value": "merges.txt: 100%"
          }
        },
        "d84997b7658449cb850d5708d7f6aae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_551a1a7fad2e49f19d23c91a920ebbb2",
            "max": 1671839,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93f05e525b7f4bdcad7e7b2224c4af33",
            "value": 1671839
          }
        },
        "31cf726d606a4708ada81e2d028061b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d66b46d98514a3183117deea0dcbc4c",
            "placeholder": "​",
            "style": "IPY_MODEL_a53de60338a24ed3abe5369b3064872f",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 1.92MB/s]"
          }
        },
        "702d7e5e362148709809c04d10301a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08d09d7236164e41ba3d06f8cb487cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff3aeab38e91462da137930ddcdc8bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "551a1a7fad2e49f19d23c91a920ebbb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f05e525b7f4bdcad7e7b2224c4af33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d66b46d98514a3183117deea0dcbc4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a53de60338a24ed3abe5369b3064872f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c73052e08fc14732b065a6a3f0f92fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b69450f87e944608b625e8223d8512f",
              "IPY_MODEL_21a028881aa44dad9f86853c432fe0d4",
              "IPY_MODEL_f67dbd7180cb498eb127d37a7e8e0859"
            ],
            "layout": "IPY_MODEL_d390028fd15e4ac59950ceb579b2e283"
          }
        },
        "1b69450f87e944608b625e8223d8512f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6586fa7990e146fb9b752cdc79b1ed7f",
            "placeholder": "​",
            "style": "IPY_MODEL_cca49a29536d4e66a57c58892628347e",
            "value": "tokenizer.json: 100%"
          }
        },
        "21a028881aa44dad9f86853c432fe0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3feab53c537c484ca1082561fcb51cd8",
            "max": 7031645,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7dff0ab5fbb94ebcb4ac8b6c4195397f",
            "value": 7031645
          }
        },
        "f67dbd7180cb498eb127d37a7e8e0859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c756fd0143ae4e429ecacc1f6f510a53",
            "placeholder": "​",
            "style": "IPY_MODEL_f96df489063146f8954befcbaaec06ea",
            "value": " 7.03M/7.03M [00:01&lt;00:00, 6.45MB/s]"
          }
        },
        "d390028fd15e4ac59950ceb579b2e283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6586fa7990e146fb9b752cdc79b1ed7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cca49a29536d4e66a57c58892628347e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3feab53c537c484ca1082561fcb51cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dff0ab5fbb94ebcb4ac8b6c4195397f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c756fd0143ae4e429ecacc1f6f510a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f96df489063146f8954befcbaaec06ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}